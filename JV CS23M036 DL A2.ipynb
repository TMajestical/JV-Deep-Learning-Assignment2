{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf5ab33-3dd0-4892-86d1-30a5b5647b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78650119-e33a-4460-a456-bfcc7ff99de4",
   "metadata": {},
   "source": [
    "I would develop the code for the assignment in this notebook as it is easy to quickly test (and even unit testing).\n",
    "\n",
    "When a module/part is bug free I would add it to the .py file later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf443ed0-08d3-4cb9-939e-0f89866253c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88764d7a-de7d-4027-880e-9958a89230de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(dir,returnIfDirAlreadyExists=False):\n",
    "    \"\"\"\n",
    "    Function to create a directory, if it doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(dir)\n",
    "    except Exception as e:\n",
    "        if \"File exists\" in str(e):\n",
    "            if returnIfDirAlreadyExists:\n",
    "                return True\n",
    "            pass\n",
    "        else:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14a836f-817b-4317-9728-9629874d1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 76 #setting this as seed wherever randomness comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5bac8d-ac8a-43a8-9eba-235fa5688183",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train data downloaded from the given source (https://storage.googleapis.com/wandb_datasets/nature_12K.zip)\n",
    "\n",
    "\"\"\"\n",
    "Now, the goal is to split 20% of train data, in \"train\" folder to get validation data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "data_base_dir = 'inaturalist_12K/'\n",
    "\n",
    "def train_validation_split(base_dir,seed = 76):\n",
    "    \"\"\"\n",
    "    Function to split 20% of the train data into validation data, Uniformly At Random (UAR). Import os and shutil before using this method.\n",
    "\n",
    "    Note  : Instead of taking 20% of samples randomly out of the entire train data; 20% of train data of each class is taken (UAR), \n",
    "    so that for training there is a balance between the number of samples per class.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        base_dir : The path to the directory in which the \"train/\" and \"test/\" directories are present after unzipping. It is assumed that the given dir path string has a \"/ at the end.\n",
    "\n",
    "        seed : The seed use in the random number generator, default : 76.\n",
    "\n",
    "    Returns :\n",
    "\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    base_data_dir = base_dir\n",
    "    train_base_dir = base_data_dir+'train/'\n",
    "    train_data_class_dirs = os.listdir(train_base_dir)\n",
    "    \n",
    "    ## remove dirs starting with \".\" from the list\n",
    "    train_data_class_dirs = [i for i in train_data_class_dirs if i[0] != \".\" ]\n",
    "\n",
    "    ## Test data is called as val, which is confusing, hence renaming it to test\n",
    "    os.rename(data_base_dir+\"val/\",base_data_dir+\"test/\")\n",
    "    \n",
    "    \n",
    "    ## validation dir\n",
    "    val_base_dir = base_data_dir+'validation/'\n",
    "    make_dir(val_base_dir)\n",
    "    \n",
    "    ## Iterate over each class and\n",
    "    ## take 20% data of each class at random as validation data\n",
    "    \n",
    "    random_num_generator = np.random.RandomState(seed)\n",
    "    \n",
    "    for class_label in train_data_class_dirs:\n",
    "    \n",
    "        current_class_train_filenames = os.listdir(train_base_dir+class_label+\"/\")\n",
    "    \n",
    "        num_of_files = len(current_class_train_filenames)\n",
    "        \n",
    "        validation_indices = random_num_generator.choice(num_of_files,int(0.2*num_of_files),replace=False)\n",
    "        train_indices = np.array(list(set(np.arange(num_of_files)).difference(set(validation_indices))))\n",
    "    \n",
    "        ##create class dir validation dir\n",
    "        cur_validation_dir = val_base_dir + class_label +\"/\"\n",
    "        make_dir(cur_validation_dir)\n",
    "        \n",
    "        for i in validation_indices:\n",
    "            shutil.move(train_base_dir+class_label+\"/\"+current_class_train_filenames[i],cur_validation_dir+current_class_train_filenames[i])\n",
    "        \n",
    "        print(f\"Validation Split for {class_label} is Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4e6c7c-450c-4a24-9936-d20fe7bcaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Careful perform this train-validation split only once in the entire lifetime, that too on the unzipped dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "base_data_dir = \"inaturalist_12K/\"\n",
    "\n",
    "#train_validation_split(base_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aa94bc-98ae-4cf9-8d68-0c445f51ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create loader for train, test and validation data \"\"\"\n",
    "\n",
    "## Train data\n",
    "train_path = base_data_dir+\"train/\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_path,transform=torchvision.transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "\n",
    "## Validation data\n",
    "val_path = base_data_dir+\"validation/\"\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=val_path,transform=torchvision.transforms.ToTensor())\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "\n",
    "## Test data\n",
    "test_path = base_data_dir+\"test/\"\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_path,transform=torchvision.transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=16,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed26168-a782-4dca-ba95-e49bd4ddecbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d10f3-ca68-494f-a842-540a25e4e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class (that inherits nn.Module), to create the CNN architecture as required and to define the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,input_depth,num_output_neurons, activation, convolution_layer_specifications, hidden_layer_specifications, output_activation):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            input_depth : Depth of the input image (number of channels).\n",
    "            \n",
    "            num_output_neurons: Number of neurons in the output layer.\n",
    "\n",
    "            activation : A torch nn method to be used as activation function.\n",
    "            \n",
    "            convolution_layer_specifications: A list of lists. There exists one list per conv. layer contaning, containing number of filters, filter sizes, paddings.\n",
    "            \n",
    "            hidden_layer_specifications: A list of ints. Number of elements correspond to number of hidden layers and each value gives the number of neurons in the corresponding hidden layer.\n",
    "            \n",
    "            output_activation: The torch nn activation method to be used for the output layer.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "            None.\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "        self.convolutional_layers = nn.ModuleList() ## Create a module list to organize the convolutional layers\n",
    "        input_channels = input_depth\n",
    "        \n",
    "        ## iterate over the convolution_layer_specifications and create the convolutional layers accordingly\n",
    "        for number_of_filters, filter_size, stride, padding, max_pool_dim in convolution_layer_specifications:\n",
    "\n",
    "            ## Assuming filter is a square matrix, so filter_size is int.\n",
    "            convolutional_layers.append(nn.Conv2d(input_channels, number_of_filters, filter_size, stride, padding))\n",
    "            convolutional_layers.append(activation)  # Add activation after each convolutional layer\n",
    "            convolutional_layers.append(nn.MaxPool2d(max_pool_dim))\n",
    "            \n",
    "            input_channels = number_of_filters  # Update input depth for next layer\n",
    "\n",
    "\n",
    "        ## iterate over the hidden_layer_specifications and create CNN layers accordingly.\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        fan_in = \"\"\"What should come here?\"\"\" ## interface betweeon maxpooling and dense layer\n",
    "        for hidden_size in hidden_layer_specifications\n",
    "            hidden_layers.append(nn.Linear(fan_in, hidden_size))\n",
    "            hidden_layers.append(nn.ReLU())  # Add ReLU activation after each dense layer\n",
    "            fan_in = hidden_size  # Update number of input features for next layer\n",
    "\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_layer_specifications[-1], num_output_neurons)\n",
    "\n",
    "\n",
    "        def forward(self,x):\n",
    "\n",
    "            ## pass through convolution, activation, pooling layer set\n",
    "            for layer in self.convolutional_layers:\n",
    "                x = layer(x)\n",
    "            \n",
    "            x = torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "            ## pass through hidden layers\n",
    "            for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "            ## compute and activate the output\n",
    "            output = self.output_activation(self.output_layer(x))\n",
    "            \n",
    "            return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406946e-93ef-40a5-91e0-19041d09fc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8182f8-b5cf-45ba-bf1f-5e2f61f8e8f3",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "2. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#:~:text=PyTorch%20provides%20two%20data%20primitives,easy%20access%20to%20the%20samples.\n",
    "3. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "4. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fa0fb-c54e-45f6-ad67-e09765be08c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
