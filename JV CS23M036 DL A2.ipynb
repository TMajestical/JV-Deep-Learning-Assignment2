{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf5ab33-3dd0-4892-86d1-30a5b5647b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78650119-e33a-4460-a456-bfcc7ff99de4",
   "metadata": {},
   "source": [
    "I would develop the code for the assignment in this notebook as it is easy to quickly test (and even unit testing).\n",
    "\n",
    "When a module/part is bug free I would add it to the .py file later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf443ed0-08d3-4cb9-939e-0f89866253c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88764d7a-de7d-4027-880e-9958a89230de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(dir,returnIfDirAlreadyExists=False):\n",
    "    \"\"\"\n",
    "    Function to create a directory, if it doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(dir)\n",
    "    except Exception as e:\n",
    "        if \"File exists\" in str(e):\n",
    "            if returnIfDirAlreadyExists:\n",
    "                return True\n",
    "            pass\n",
    "        else:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14a836f-817b-4317-9728-9629874d1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 76 #setting this as seed wherever randomness comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5bac8d-ac8a-43a8-9eba-235fa5688183",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train data downloaded from the given source (https://storage.googleapis.com/wandb_datasets/nature_12K.zip)\n",
    "\n",
    "\"\"\"\n",
    "Now, the goal is to split 20% of train data, in \"train\" folder to get validation data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "data_base_dir = 'inaturalist_12K/'\n",
    "\n",
    "def train_validation_split(base_dir,seed = 76):\n",
    "    \"\"\"\n",
    "    Function to split 20% of the train data into validation data, Uniformly At Random (UAR). Import os and shutil before using this method.\n",
    "\n",
    "    Note  : Instead of taking 20% of samples randomly out of the entire train data; 20% of train data of each class is taken (UAR), \n",
    "    so that for training there is a balance between the number of samples per class.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        base_dir : The path to the directory in which the \"train/\" and \"test/\" directories are present after unzipping. It is assumed that the given dir path string has a \"/ at the end.\n",
    "\n",
    "        seed : The seed use in the random number generator, default : 76.\n",
    "\n",
    "    Returns :\n",
    "\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    base_data_dir = base_dir\n",
    "    train_base_dir = base_data_dir+'train/'\n",
    "    train_data_class_dirs = os.listdir(train_base_dir)\n",
    "    \n",
    "    ## remove dirs starting with \".\" from the list\n",
    "    train_data_class_dirs = [i for i in train_data_class_dirs if i[0] != \".\" ]\n",
    "\n",
    "    ## Test data is called as val, which is confusing, hence renaming it to test\n",
    "    os.rename(data_base_dir+\"val/\",base_data_dir+\"test/\")\n",
    "    \n",
    "    \n",
    "    ## validation dir\n",
    "    val_base_dir = base_data_dir+'validation/'\n",
    "    make_dir(val_base_dir)\n",
    "    \n",
    "    ## Iterate over each class and\n",
    "    ## take 20% data of each class at random as validation data\n",
    "    \n",
    "    random_num_generator = np.random.RandomState(seed)\n",
    "    \n",
    "    for class_label in train_data_class_dirs:\n",
    "    \n",
    "        current_class_train_filenames = os.listdir(train_base_dir+class_label+\"/\")\n",
    "    \n",
    "        num_of_files = len(current_class_train_filenames)\n",
    "        \n",
    "        validation_indices = random_num_generator.choice(num_of_files,int(0.2*num_of_files),replace=False)\n",
    "        train_indices = np.array(list(set(np.arange(num_of_files)).difference(set(validation_indices))))\n",
    "    \n",
    "        ##create class dir validation dir\n",
    "        cur_validation_dir = val_base_dir + class_label +\"/\"\n",
    "        make_dir(cur_validation_dir)\n",
    "        \n",
    "        for i in validation_indices:\n",
    "            shutil.move(train_base_dir+class_label+\"/\"+current_class_train_filenames[i],cur_validation_dir+current_class_train_filenames[i])\n",
    "        \n",
    "        print(f\"Validation Split for {class_label} is Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4e6c7c-450c-4a24-9936-d20fe7bcaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Careful perform this train-validation split only once in the entire lifetime, that too on the unzipped dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "base_data_dir = \"inaturalist_12K/\"\n",
    "\n",
    "#train_validation_split(base_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aa94bc-98ae-4cf9-8d68-0c445f51ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create loader for train, test and validation data \"\"\"\n",
    "\n",
    "## Train data\n",
    "train_path = base_data_dir+\"train/\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_path,transform=torchvision.transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "\n",
    "## Validation data\n",
    "val_path = base_data_dir+\"validation/\"\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=val_path,transform=torchvision.transforms.ToTensor())\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "\n",
    "## Test data\n",
    "test_path = base_data_dir+\"test/\"\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_path,transform=torchvision.transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=16,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26849df9-9cf4-4d45-b9dd-9f8048fd3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some important points:\n",
    "\n",
    "1. Images are of different shapes, so they all have to be resized by looking at the image shapes and picking the maximum [or mean/mode ?] width and height.\n",
    "\n",
    "2. Now they have to be normalized. To zero mean and unit variance. This has to be done within train, test and validation datasets separately. One normalization won't work for the other because the images may go out of scale (i.e beyond +/- 1)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed26168-a782-4dca-ba95-e49bd4ddecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_image_sizes(ds):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(len(ds.samples)):\n",
    "        \n",
    "        x,y = train_dataset[i][0].shape[1:]\n",
    "    \n",
    "        x_vals.append(x)\n",
    "        y_vals.append(y)\n",
    "        \n",
    "        #images.append(train_dataset[i][0])\n",
    "\n",
    "    print(f\"X min : {np.min(x_vals)}\\tX Max:{np.max(x_vals)}\")    \n",
    "    print(f\"Y min : {np.min(y_vals)}\\tY Max:{np.max(y_vals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8726df22-8dbd-44c2-aba0-2dae72b12a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X min : 800\tX Max:800\n",
      "Y min : 800\tY Max:800\n"
     ]
    }
   ],
   "source": [
    "viz_image_sizes(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07e142e5-5dcf-4f4d-94a5-fcb720a0a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X min : 800\tX Max:800\n",
      "Y min : 800\tY Max:800\n",
      "X min : 800\tX Max:800\n",
      "Y min : 800\tY Max:800\n"
     ]
    }
   ],
   "source": [
    "viz_image_sizes(val_dataset)\n",
    "viz_image_sizes(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b35015-f63c-4123-9252-3a66177e4edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs the maximum dimensions are 800,800 for train, validatio and test data\\n\\nResizing all images to 800x800 size.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "As the maximum dimensions are 800,800 for train, validatio and test data\n",
    "\n",
    "Resizing all images to 800x800 size.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d00e0b-4ac6-44e1-acb0-9dcdf81b2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train data\n",
    "\n",
    "\"\"\"data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(800,800)),\n",
    "        transforms.Normalize(mean=[0,0,0],std=[1,1,1]),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\"\"\"\n",
    "\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(800,800)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_path = base_data_dir+\"train/\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_path,transform=data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "\n",
    "## Validation data\n",
    "val_path = base_data_dir+\"validation/\"\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=val_path,transform=data_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "\n",
    "## Test data\n",
    "test_path = base_data_dir+\"test/\"\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_path,transform=data_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=16,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e602f83c-0351-43c4-ad87-8d2f714fd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57760de8-14b9-437b-9741-a62c83c023e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Initial shape : 227x227x3\n",
      "Number of Parameters : 34848\n",
      "\n",
      "Shape after conv : 55x55x96\n",
      "Shape after MP : 27x27x96\n",
      "Number of Parameters : 614400\n",
      "\n",
      "Shape after conv : 23x23x256\n",
      "Shape after MP : 11x11x256\n",
      "Number of Parameters : 884736\n",
      "\n",
      "Shape after conv : 9x9x384\n",
      "Number of Parameters : 1327104\n",
      "\n",
      "Shape after conv : 7x7x384\n",
      "Number of Parameters : 884736\n",
      "\n",
      "Shape after conv : 5x5x256\n",
      "Shape after MP : 2x2x256\n",
      "\n",
      "Final Shape : 2x2x256\n",
      "Total Convolutional params : 3745824\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Iterate over:\n",
    "\n",
    "number_of_filters, filter_size, stride, padding, max_pool_dim in convolution_layer_specifications:\n",
    "\n",
    "and given the input image size H and W, compute the output size after last convolution.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "convolution_layer_specifications = [\n",
    "\n",
    "    [\"conv\",6,5,1,0],\n",
    "    [\"maxpool\",2,2],\n",
    "    [\"conv\",16,5,1,0],\n",
    "    [\"maxpool\",2,2]\n",
    "]\n",
    "\n",
    "\n",
    "convolution_layer_specifications = [\n",
    "\n",
    "    [\"conv\",96,11,4,0],\n",
    "    [\"maxpool\",3,2],\n",
    "    [\"conv\",256,5,1,0],\n",
    "    [\"maxpool\",3,2],\n",
    "    [\"conv\",384,3,1,0],\n",
    "    [\"conv\",384,3,1,0],\n",
    "    [\"conv\",256,3,1,0],\n",
    "    [\"maxpool\",3,2]\n",
    "]\n",
    "\n",
    "W=227\n",
    "H=227\n",
    "\n",
    "W_cur = W\n",
    "H_cur = H\n",
    "depth_cur = 3\n",
    "\n",
    "total_params = 0\n",
    "\n",
    "print(f\"Shape Initial shape : {W_cur}x{H_cur}x{depth_cur}\")\n",
    "\n",
    "for config in convolution_layer_specifications: ## assuming square filters\n",
    "\n",
    "    #print(number_of_filters, filter_size, stride, padding, max_pool)\n",
    "    type = config[0]\n",
    "\n",
    "    if type == \"conv\":  ## its a convolutional layer\n",
    "\n",
    "        \n",
    "        _, number_of_filters, filter_size, stride, padding = config\n",
    "        \n",
    "        W_cur = (W_cur - filter_size + 2*padding)//stride + 1\n",
    "        H_cur = (H_cur - filter_size + 2*padding)//stride + 1\n",
    "\n",
    "        params = filter_size*filter_size*depth_cur*number_of_filters\n",
    "        total_params += params\n",
    "        \n",
    "        print(f\"Number of Parameters : {params}\\n\")\n",
    "        depth_cur = number_of_filters\n",
    "        \n",
    "        print(f\"Shape after conv : {W_cur}x{H_cur}x{depth_cur}\")\n",
    "\n",
    "    elif type == \"maxpool\":\n",
    "        _,max_pool_filter_size,max_pool_stride = config\n",
    "\n",
    "        W_cur = (W_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "        H_cur = (H_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "        print(f\"Shape after MP : {W_cur}x{H_cur}x{depth_cur}\")\n",
    "\n",
    "\n",
    "print(f\"\\nFinal Shape : {W_cur}x{H_cur}x{depth_cur}\")\n",
    "\n",
    "print(f\"Total Convolutional params : {total_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d564e3-585b-407c-a2aa-f86b74d68e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c59d10f3-ca68-494f-a842-540a25e4e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class (that inherits nn.Module), to create the CNN architecture as required and to define the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_output_dimension_post_convolutions(self,convolution_layer_specifications):\n",
    "            \n",
    "            \"\"\"\n",
    "            Method to compute the dimension of the output after all convolution,maxpooling layers.\n",
    "            This helps to compute the input size of first fully connected layer.\n",
    "\n",
    "            Params:\n",
    "                convolution_layer_specifications :  A list of lists. There exists one list per conv or maxpool. \n",
    "\n",
    "                        The first element of the list is a string \"conv\" or \"maxpooling\". Indicating the layer type\n",
    "                \"conv\" is followed by number of filters, filter sizes, stride, paddings. It is assumed that every convolutional layer is followed by an activation layer.\n",
    "                \"maxpool\" is followed by filter size and stride.\n",
    "            \n",
    "            Returns:\n",
    "                A tuple containing the dimension (Height,Width,depth) of the output after all convolutional operations.\n",
    "            \"\"\"\n",
    "\n",
    "            W_cur = self.W\n",
    "            H_cur = self.H\n",
    "            depth_cur = self.input_channels\n",
    "            \n",
    "            for config in convolution_layer_specifications: ## assuming square filters\n",
    "            \n",
    "                type = config[0]\n",
    "            \n",
    "                if type == \"conv\":  ## its a convolutional layer\n",
    "            \n",
    "                    \n",
    "                    _, number_of_filters, filter_size, stride, padding = config\n",
    "                    \n",
    "                    W_cur = (W_cur - filter_size + 2*padding)//stride + 1\n",
    "                    H_cur = (H_cur - filter_size + 2*padding)//stride + 1\n",
    "                    depth_cur = number_of_filters\n",
    "                    \n",
    "                elif type == \"maxpool\":\n",
    "                    _,max_pool_filter_size,max_pool_stride = config\n",
    "            \n",
    "                    W_cur = (W_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "                    H_cur = (H_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "            \n",
    "\n",
    "            return (H_cur,W_cur,depth_cur)\n",
    "\n",
    "    def initalize_weights_biases(self,m):\n",
    "        \"\"\"\n",
    "        \n",
    "        Method to initialize weights given a torch module.\n",
    "\n",
    "        Using \"HE\" (kaiming_normal) Initialization, as it goes well with ReLU in CNN.\n",
    "        \n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):  ## it its a fully connected layer\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.01) ##a small non-zero value\n",
    "\n",
    "        elif isinstance(m, nn.Conv2d): ## if its a Convolutional layer\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.01) ##a small non-zero value\n",
    "\n",
    "        ## Maxpool and activation layers would not have any parameters, so no initialization for them\n",
    "    \n",
    "    def __init__(self,num_output_neurons, activation, convolution_layer_specifications, hidden_layer_specifications, output_activation, W=800, H=800, input_depth=3):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            num_output_neurons: Number of neurons in the output layer.\n",
    "\n",
    "            activation : A torch nn method to be used as activation function.\n",
    "            \n",
    "            convolution_layer_specifications: A list of lists. There exists one list per conv or maxpool. \n",
    "\n",
    "                        The first element of the list is a string \"conv\" or \"maxpooling\". Indicating the layer type\n",
    "                \"conv\" is followed by number of filters, filter sizes, stride, paddings. It is assumed that every convolutional layer is followed by an activation layer.\n",
    "                \"maxpool\" is followed by filter size and stride.\n",
    "            \n",
    "            hidden_layer_specifications: A list of ints. Number of elements correspond to number of hidden layers and each value gives the number of neurons in the corresponding hidden layer.\n",
    "            \n",
    "            output_activation: The torch nn activation method to be used for the output layer.\n",
    "\n",
    "            H : Height of the input image. [should be fixed for the dataset] (default:800)\n",
    "\n",
    "            W : Width of the input image.  [should be fixed for the dataset]  (default:800)\n",
    "\n",
    "            input_depth : Depth of the input image (number of channels). [should be fixed for the dataset] (default:3)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "            None.\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.input_channels = input_depth\n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "        self.convolutional_layers = nn.ModuleList() ## Create a module list to organize the convolutional layers\n",
    "        \n",
    "        \n",
    "        ## iterate over the convolution_layer_specifications and create the convolutional layers accordingly\n",
    "\n",
    "        cur_depth = self.input_channels\n",
    "        for config in convolution_layer_specifications:\n",
    "\n",
    "            ## Assuming filter is a square matrix, so filter_size is int.\n",
    "            type = config[0]\n",
    "\n",
    "            if type == \"conv\":\n",
    "                _, number_of_filters, filter_size, stride, padding = config\n",
    "                self.convolutional_layers.append(nn.Conv2d(cur_depth, number_of_filters, filter_size, stride, padding))\n",
    "                self.convolutional_layers.append(activation)  # Add activation after each convolutional layer\n",
    "            \n",
    "            elif type == \"maxpool\":\n",
    "                _,max_pool_filter_size,max_pool_stride = config\n",
    "                self.convolutional_layers.append(nn.MaxPool2d(max_pool_filter_size,max_pool_stride))\n",
    "            \n",
    "            cur_depth = number_of_filters  # Update input depth for next layer\n",
    "\n",
    "        self.convolutional_layers.apply(self.initalize_weights_biases)\n",
    "        \n",
    "        ## iterate over the hidden_layer_specifications and create CNN layers accordingly.\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        conv_output_height,conv_output_height,conv_output_depth = self.compute_output_dimension_post_convolutions(convolution_layer_specifications)\n",
    "        \n",
    "        fan_in =  conv_output_height*conv_output_height*conv_output_depth ## interface betweeon maxpooling and dense layer\n",
    "        \n",
    "        for hidden_size in hidden_layer_specifications:\n",
    "            self.hidden_layers.append(nn.Linear(fan_in, hidden_size))\n",
    "            self.hidden_layers.append(nn.ReLU())  # Add ReLU activation after each dense layer\n",
    "            fan_in = hidden_size  # Update number of input features for next layer\n",
    "\n",
    "        self.hidden_layers.apply(self.initalize_weights_biases)\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_layer_specifications[-1], num_output_neurons)\n",
    "\n",
    "        self.output_layer.apply(self.initalize_weights_biases)\n",
    "\n",
    "\n",
    "        def forward(self,x):\n",
    "\n",
    "            ## pass through convolution, activation, pooling layer set\n",
    "            for layer in self.convolutional_layers:\n",
    "                x = layer(x)\n",
    "            \n",
    "            x = torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "            ## pass through hidden layers\n",
    "            for layer in self.dense_layers:\n",
    "                x = layer(x)\n",
    "\n",
    "            ## compute and activate the output\n",
    "            output = self.output_activation(self.output_layer(x))\n",
    "            \n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4406946e-93ef-40a5-91e0-19041d09fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes =  len(train_dataset.classes)\n",
    "\n",
    "convolution_layer_specifications = [\n",
    "\n",
    "    [\"conv\",96,11,4,0],\n",
    "    [\"maxpool\",3,2],\n",
    "    [\"conv\",256,5,1,0],\n",
    "    [\"maxpool\",3,2],\n",
    "    [\"conv\",384,3,1,0],\n",
    "    [\"conv\",384,3,1,0],\n",
    "    [\"conv\",256,3,1,0],\n",
    "    [\"maxpool\",3,2]\n",
    "]\n",
    "\n",
    "hidden_layer_specifications = [256]\n",
    "\n",
    "\n",
    "\n",
    "model = CNN(num_output_neurons=num_classes, activation=nn.ReLU(), convolution_layer_specifications=convolution_layer_specifications, hidden_layer_specifications=hidden_layer_specifications, output_activation = nn.Softmax(), W=800, H=800, input_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4f4c766-598c-438f-9081-67c0e3c00a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_network_architecture(model):\n",
    "    print(model)\n",
    "    print(\"Network Architecture:\")\n",
    "    for name, module in model.named_children():\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Layer Name: {name}\")\n",
    "        print(module)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b3c0a59-95b1-461d-b6ad-2f284de9c0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (output_activation): Softmax(dim=None)\n",
      "  (convolutional_layers): ModuleList(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=92416, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Network Architecture:\n",
      "----------------------------------------\n",
      "Layer Name: output_activation\n",
      "Softmax(dim=None)\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Layer Name: convolutional_layers\n",
      "ModuleList(\n",
      "  (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (7): ReLU()\n",
      "  (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (11): ReLU()\n",
      "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Layer Name: hidden_layers\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=92416, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Layer Name: output_layer\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_network_architecture(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099eacc-cd5f-43e1-b66b-4a067ee66be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    [\"conv\",6,5,1,0],\n",
    "    [\"maxpool\",2,2],\n",
    "    [\"conv\",16,5,1,0],\n",
    "    [\"maxpool\",2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517e983-9f3b-4716-b44b-936371ddfc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8182f8-b5cf-45ba-bf1f-5e2f61f8e8f3",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "2. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#:~:text=PyTorch%20provides%20two%20data%20primitives,easy%20access%20to%20the%20samples.\n",
    "3. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "4. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "5. https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n",
    "6. https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html\n",
    "7. https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fa0fb-c54e-45f6-ad67-e09765be08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def get_train_valid_loader(data_dir,\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           img_size,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True,\n",
    "                           show_sample=False,\n",
    "                           num_workers=4,\n",
    "                           pin_memory=False):\n",
    "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
    "    Params\n",
    "    ------\n",
    "    - data_dir: path directory to the dataset.\n",
    "    - batch_size: how many samples per batch to load.\n",
    "    - augment: whether to apply the data augmentation scheme\n",
    "      mentioned in the paper. Only applied on the train split.\n",
    "    - random_seed: fix seed for reproducibility.\n",
    "    - valid_size: percentage split of the training set used for\n",
    "      the validation set. Should be a float in the range [0, 1].\n",
    "    - shuffle: whether to shuffle the train/validation indices.\n",
    "    - show_sample: plot 9x9 sample grid of the dataset.\n",
    "    - num_workers: number of subprocesses to use when loading the dataset.\n",
    "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
    "      True if using GPU.\n",
    "    Returns\n",
    "    -------\n",
    "    - train_loader: training set iterator.\n",
    "    - valid_loader: validation set iterator.\n",
    "    \n",
    "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
    "    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
    "\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.RandomHorizontalFlip(0.3),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=train_dir,\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.RandomHorizontalFlip(0.3),\n",
    "            transforms.ToTensor()])\n",
    "    )\n",
    "\n",
    "    valid_dataset = train_dataset = datasets.ImageFolder(\n",
    "        root=train_dir,\n",
    "        transform=valid_transform\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    # visualize some images\n",
    "    if show_sample:\n",
    "        sample_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=9, shuffle=shuffle,\n",
    "            num_workers=num_workers, pin_memory=pin_memory,\n",
    "        )\n",
    "        data_iter = iter(sample_loader)\n",
    "        images, labels = data_iter.next()\n",
    "        X = images.numpy().transpose([0, 2, 3, 1])\n",
    "        plot_images(X, labels)\n",
    "\n",
    "    return (train_loader, valid_loader)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
