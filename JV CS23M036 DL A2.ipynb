{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would develop the code for the assignment in this notebook as it is easy to quickly test (and even unit testing).\n",
    "\n",
    "When a module/part is bug free I would add it to the .py file later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,ChainDataset, ConcatDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 76 #setting this as seed wherever randomness comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sizes(ds):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(len(ds.samples)):\n",
    "        \n",
    "        x,y = train_dataset[i][0].shape[1:]\n",
    "    \n",
    "        x_vals.append(x)\n",
    "        y_vals.append(y)\n",
    "        \n",
    "        #images.append(train_dataset[i][0])\n",
    "\n",
    "    print(f\"X min : {np.min(x_vals)}\\tX Max:{np.max(x_vals)}\")    \n",
    "    print(f\"Y min : {np.min(y_vals)}\\tY Max:{np.max(y_vals)}\")\n",
    "\n",
    "#get_image_sizes(train_dataset)\n",
    "#get_image_sizes(val_dataset)\n",
    "#get_image_sizes(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important points:\n",
    "\n",
    "1. Images are of different shapes, so they all have to be resized by looking at the image shapes and picking the maximum [or mean/mode ?] width and height.\n",
    "\n",
    "2. Now they have to be normalized. To zero mean and unit variance. This has to be done within train, test and validation datasets separately. One normalization won't work for the other because the images may go out of scale (i.e beyond +/- 1)\n",
    "\n",
    "\n",
    "As the maximum dimensions are 800,800 for train, validatio and test data\n",
    "\n",
    "Resizing all images to 800x800 size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class (that inherits nn.Module), to create the CNN architecture as required and to define the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_output_dimension_post_convolutions(self,convolution_layer_specifications):\n",
    "            \n",
    "            \"\"\"\n",
    "            Method to compute the dimension of the output after all convolution,maxpooling layers.\n",
    "            This helps to compute the input size of first fully connected layer.\n",
    "\n",
    "            Params:\n",
    "                convolution_layer_specifications :  A list of lists. There exists one list per conv or maxpool. \n",
    "\n",
    "                        The first element of the list is a string \"conv\" or \"maxpooling\". Indicating the layer type\n",
    "                \"conv\" is followed by number of filters, filter sizes, stride, paddings. It is assumed that every convolutional layer is followed by an activation layer.\n",
    "                \"maxpool\" is followed by filter size and stride.\n",
    "            \n",
    "            Returns:\n",
    "                A tuple containing the dimension (Height,Width,depth) of the output after all convolutional operations.\n",
    "            \"\"\"\n",
    "\n",
    "            W_cur = self.W\n",
    "            H_cur = self.H\n",
    "            depth_cur = self.input_channels\n",
    "            \n",
    "            for config in convolution_layer_specifications: ## assuming square filters\n",
    "            \n",
    "                layer_type = config[0]\n",
    "            \n",
    "                if layer_type == \"conv\":  ## its a convolutional layer\n",
    "            \n",
    "                    \n",
    "                    _, number_of_filters, filter_size, stride, padding = config\n",
    "                    \n",
    "                    W_cur = (W_cur - filter_size + 2*padding)//stride + 1\n",
    "                    H_cur = (H_cur - filter_size + 2*padding)//stride + 1\n",
    "                    depth_cur = number_of_filters\n",
    "                    \n",
    "                elif layer_type == \"maxpool\":\n",
    "                    _,max_pool_filter_size,max_pool_stride = config\n",
    "            \n",
    "                    W_cur = (W_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "                    H_cur = (H_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "            \n",
    "\n",
    "            return (H_cur,W_cur,depth_cur)\n",
    "\n",
    "    \n",
    "    def __init__(self,num_output_neurons, conv_activation, fc_activation, convolution_layer_specifications, hidden_layer_specifications, output_activation,conv_batch_norm = False,fc_batch_norm = False, dropout = None,W=800, H=800, input_depth=3):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            num_output_neurons: Number of neurons in the output layer.\n",
    "\n",
    "            conv_activation : A torch nn method to be used as activation function after the convolutional layers.\n",
    "            \n",
    "            fc_activation : A torch nn method to be used as activation function for the fully connected layers.\n",
    "            \n",
    "            convolution_layer_specifications: A list of lists. There exists one list per conv or maxpool. \n",
    "\n",
    "                        The first element of the list is a string \"conv\" or \"maxpooling\". Indicating the layer type\n",
    "                \"conv\" is followed by number of filters, filter sizes, stride, paddings. It is assumed that every convolutional layer is followed by an activation layer.\n",
    "                \"maxpool\" is followed by filter size and stride.\n",
    "            \n",
    "            hidden_layer_specifications: A list of ints. Number of elements correspond to number of hidden layers and each value gives the number of neurons in the corresponding hidden layer.\n",
    "            \n",
    "            output_activation: The torch nn activation method to be used for the output layer.\n",
    "\n",
    "            batch_norm  : Applies batch norm after each Convolutional Layer or Fully Connected layer.\n",
    "\n",
    "            dropout :  The dropout probability (float), default is None.\n",
    "\n",
    "            H : Height of the input image. [should be fixed for the dataset] (default:800)\n",
    "\n",
    "            W : Width of the input image.  [should be fixed for the dataset]  (default:800)\n",
    "\n",
    "            input_depth : Depth of the input image (number of channels). [should be fixed for the dataset] (default:3)\n",
    "\n",
    "\n",
    "            Note:\n",
    "                Dropout is a regularization method, and Convolutional layers by construction have sparse and shared weights so they have regularization inbuilt,\n",
    "\n",
    "                hence dropout is applied only to the fully connected layers.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "            None.\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.input_channels = input_depth\n",
    "        self.output_activation = output_activation\n",
    "        self.convolution_layer_specifications = convolution_layer_specifications\n",
    "        \n",
    "        self.convolutional_layers = nn.ModuleList() ## Create a module list to organize the convolutional layers\n",
    "        \n",
    "        self.dropout  = dropout\n",
    "        \n",
    "        ## iterate over the convolution_layer_specifications and create the convolutional layers accordingly\n",
    "\n",
    "        cur_depth = self.input_channels\n",
    "        for config in convolution_layer_specifications:\n",
    "\n",
    "            ## Assuming filter is a square matrix, so filter_size is int.\n",
    "            type = config[0]\n",
    "\n",
    "            if type == \"conv\":\n",
    "                _, number_of_filters, filter_size, stride, padding = config\n",
    "                self.convolutional_layers.append(nn.Conv2d(cur_depth, number_of_filters, filter_size, stride, padding))\n",
    "                ## If batchnorm is to be done for Conv layers\n",
    "                if conv_batch_norm:\n",
    "                    self.convolutional_layers.append(nn.BatchNorm2d(number_of_filters))\n",
    "                self.convolutional_layers.append(conv_activation)  # Add activation after each convolutional layer\n",
    "            \n",
    "            elif type == \"maxpool\":\n",
    "                _,max_pool_filter_size,max_pool_stride = config\n",
    "                self.convolutional_layers.append(nn.MaxPool2d(max_pool_filter_size,max_pool_stride))\n",
    "            \n",
    "            cur_depth = number_of_filters  # Update input depth for next layer\n",
    "\n",
    "        #self.convolutional_layers.apply(self.initalize_weights_biases)\n",
    "        \n",
    "        ## iterate over the hidden_layer_specifications and create CNN layers accordingly.\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        conv_output_height,conv_output_height,conv_output_depth = self.compute_output_dimension_post_convolutions(convolution_layer_specifications)\n",
    "        \n",
    "        #if self.dropout:\n",
    "            \n",
    "        #    self.dropout_layer = nn.Dropout(p=self.dropout) ## a drop out layer with dropout\n",
    "        \n",
    "        \n",
    "        fan_in =  conv_output_height*conv_output_height*conv_output_depth ## interface betweeon maxpooling and dense layer\n",
    "        \n",
    "        for hidden_size in hidden_layer_specifications:\n",
    "            self.hidden_layers.append(nn.Linear(fan_in, hidden_size))\n",
    "            \n",
    "            ## If batchnorm is to be done for FC layers\n",
    "            if fc_batch_norm:\n",
    "                self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                \n",
    "            self.hidden_layers.append(fc_activation)  # Add ReLU activation after each dense layer\n",
    "\n",
    "            if self.dropout:\n",
    "                self.hidden_layers.append(nn.Dropout(p=self.dropout))\n",
    "\n",
    "            fan_in = hidden_size  # Update number of input features for next layer\n",
    "\n",
    "        #self.hidden_layers.apply(self.initalize_weights_biases)\n",
    "\n",
    "        self.output = nn.ModuleList()\n",
    "        self.output.append(nn.Linear(hidden_layer_specifications[-1], num_output_neurons))\n",
    "        self.output.append(self.output_activation)\n",
    "        \n",
    "        #self.output.apply(self.initalize_weights_biases)\n",
    " \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        ## pass through convolution, activation, pooling layer set\n",
    "        for layer in self.convolutional_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "        ## pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        for layer in self.output: ## compute output and apply softmax\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "    def initalize_weights_biases(self,m):\n",
    "        \"\"\"\n",
    "        \n",
    "        Method to initialize weights given a torch module.\n",
    "\n",
    "        Using \"HE\" (kaiming_normal) Initialization, as it goes well with ReLU in CNN.\n",
    "        \n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):  ## it its a fully connected layer\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01) ##a small non-zero value\n",
    "\n",
    "        elif isinstance(m, nn.Conv2d): ## if its a Convolutional layer\n",
    "            torch.nn.init.kaiming_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01) ##a small non-zero value\n",
    "\n",
    "        ## Maxpool and activation layers would not have any parameters, so no initialization for them\n",
    "\n",
    "    def convolution_dimensions(self,input_channels):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to print the dimensions of the outputs of all the convolutional pooling layers\n",
    "        \"\"\"\n",
    "        \n",
    "        W_cur = self.W\n",
    "        H_cur = self.H\n",
    "        depth_cur = input_channels\n",
    "        total_params = 0\n",
    "\n",
    "        for config in self.convolution_layer_specifications: ## assuming square filters\n",
    "        \n",
    "            #print(number_of_filters, filter_size, stride, padding, max_pool)\n",
    "            layer_type = config[0]\n",
    "        \n",
    "            if layer_type == \"conv\":  ## its a convolutional layer\n",
    "        \n",
    "                \n",
    "                _, number_of_filters, filter_size, stride, padding = config\n",
    "                \n",
    "                W_cur = (W_cur - filter_size + 2*padding)//stride + 1\n",
    "                H_cur = (H_cur - filter_size + 2*padding)//stride + 1\n",
    "        \n",
    "                params = filter_size*filter_size*depth_cur*number_of_filters\n",
    "                total_params += params\n",
    "                \n",
    "                print(f\"Number of Parameters : {params}\\n\")\n",
    "                depth_cur = number_of_filters\n",
    "                \n",
    "                print(f\"Shape after conv : {W_cur}x{H_cur}x{depth_cur}\")\n",
    "        \n",
    "            elif layer_type == \"maxpool\":\n",
    "                _,max_pool_filter_size,max_pool_stride = config\n",
    "        \n",
    "                W_cur = (W_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "                H_cur = (H_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "                print(f\"Shape after MP : {W_cur}x{H_cur}x{depth_cur}\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\nFinal Shape : {W_cur}x{H_cur}x{depth_cur}\")\n",
    "        \n",
    "        print(f\"Total Convolutional params : {total_params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "\n",
    "    def __init__(self,H,W,data_dir,device,perform_standardization=False):\n",
    "\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.base_dir  = data_dir\n",
    "        self.device = device\n",
    "        self.perform_standardization = perform_standardization\n",
    "\n",
    "    def compute_mean_and_dev_in_dataset(self,sub_dir):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to compute the channel wise mean and std dev in the orignal (train/validation/test) dataset.\n",
    "\n",
    "        params:\n",
    "        \n",
    "            sub_dir : \"train/\" or \"validation/\" or \"test/\", from which data has to be taken.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            mean,std of the dataset.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.CenterCrop(size=(self.H,self.W)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        dataset = torchvision.datasets.ImageFolder(root=self.base_dir+sub_dir,transform=data_transforms)\n",
    "        loader = torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=False,num_workers=3)\n",
    "    \n",
    "        mean = 0.0\n",
    "        var = 0.0\n",
    "        count = 0\n",
    "    \n",
    "        for _, data in enumerate(loader, 0):\n",
    "    \n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device) ## move the inputs and lables to the device.\n",
    "    \n",
    "            # Reshape inputs to be the shape of [B, C, W * H]\n",
    "            # where B is the batch size, C is the number of channels in the image, and W and H are the width and height of the image respectively\n",
    "            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "    \n",
    "            # Update total number of images\n",
    "            count += inputs.size(0)\n",
    "    \n",
    "            # Compute mean and std here\n",
    "            mean += inputs.mean(2).sum(0)\n",
    "            var += inputs.var(2).sum(0)\n",
    "        \n",
    "        mean /= count\n",
    "        var /= count\n",
    "        std = torch.sqrt(var)\n",
    "    \n",
    "        #print(mean.cpu(),std.cpu())\n",
    "        \n",
    "        return mean.cpu(),std.cpu()\n",
    "\n",
    "    def create_dataloader(self,sub_dir,batch_size=32,shuffle=True,num_workers=2,data_augmentation_transforms = None,pin_memory = False):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create dataset and return dataloader after applying all necessary transforms.\n",
    "\n",
    "        params:\n",
    "\n",
    "            sub_dir : \"train/\" or \"validation/\" or \"test/\"\n",
    "            batch_size : The batch size in which training has to be performed.\n",
    "            shuffle : whether shuffling must be done before sampling.\n",
    "            num_works : Number of workers to be used on the dataset.\n",
    "            data_augmentation_transforms : Either None or List of List of transforms, with each sub-list leading to a dataset.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            Dataloader corresponding to the dataset.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Preparing data from {sub_dir}\")\n",
    "\n",
    "        default_transform = [transforms.CenterCrop(size=(self.H,self.W))]  ## to be applied always\n",
    "\n",
    "        standardization_transform = []\n",
    "        ## if data standardization needs to be done.\n",
    "        if self.perform_standardization: \n",
    "            mean,std = self.compute_mean_and_dev_in_dataset(sub_dir)\n",
    "            standardization_transform = [transforms.Normalize(mean,std)]\n",
    "\n",
    "        ## The most basic list of transforms applied to the orignal train dataset and validation and test dataset.\n",
    "        vanilla_transforms = default_transform + [transforms.ToTensor()] + standardization_transform\n",
    "\n",
    "        if (\"train\" in sub_dir) and (data_augmentation_transforms): ## if data augmentation is to be done\n",
    "            \n",
    "            original_dataset = torchvision.datasets.ImageFolder(root=self.base_dir+sub_dir,transform=transforms.Compose(vanilla_transforms))\n",
    "\n",
    "            dataset_list = [original_dataset]\n",
    "            \n",
    "            for aug_transform in data_augmentation_transforms:\n",
    "                \n",
    "                cur_data_transforms_list = default_transform + aug_transform + [transforms.ToTensor()] + standardization_transform\n",
    "                cur_dataset = torchvision.datasets.ImageFolder(root=self.base_dir+sub_dir,transform=transforms.Compose(cur_data_transforms_list))\n",
    "                dataset_list.append(cur_dataset)\n",
    "\n",
    "            self.dataset = ConcatDataset(dataset_list)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            self.dataset = torchvision.datasets.ImageFolder(root=self.base_dir+sub_dir,transform=transforms.Compose(vanilla_transforms))\n",
    "\n",
    "        \n",
    "        \n",
    "        ## Now create the data loader\n",
    "        \n",
    "        sampler = None ## unless the dataloading is distributed across devices or processes.\n",
    "        \n",
    "        if \"train\" in sub_dir:\n",
    "\n",
    "            #torch.distributed.init_process_group(rank=0,world_size = 4)\n",
    "\n",
    "            #sampler = DistributedSampler(self.dataset)\n",
    "\n",
    "            self.loader = torch.utils.data.DataLoader(dataset=self.dataset,batch_size=batch_size,shuffle=shuffle,num_workers=num_workers,pin_memory = pin_memory,sampler=sampler)\n",
    "\n",
    "        else:\n",
    "\n",
    "            num_workers = 3\n",
    "            pin_memory = False\n",
    "\n",
    "            self.loader = torch.utils.data.DataLoader(self.dataset,batch_size=batch_size,shuffle=False,num_workers=num_workers,pin_memory = pin_memory,sampler=sampler)\n",
    "            \n",
    "        return self.loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "\n",
    "    \"\"\"\n",
    "    Class to create and conduct experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,H,W,device,base_dir,wandb_logging=False,on_kaggle = False):\n",
    "\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.device = device\n",
    "        self.base_data_dir = base_dir\n",
    "        self.wandb_logging = wandb_logging\n",
    "        self.on_kaggle = on_kaggle\n",
    "\n",
    "    def create_dataloaders(self,batch_size,shuffle,perform_standardization,list_of_train_data_augmentation_transforms,num_workers=0,pin_memory=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create dataloaders for train,test and validation datasets, with the help from the DataPreparation class.\n",
    "\n",
    "        params:\n",
    "\n",
    "            batch_size : The training batch size (also applied to test and validation loaders, but anyway its still the same).\n",
    "            shuffle : True/False, whether to shuffle data before sampling.\n",
    "            perform_standardization : True/False, says if data has to be brought to zero mean and unit variance.\n",
    "            list_of_train_data_augmentation_transforms : None, if no data augmentation or List of List of transforms, with each sub-list leading to a dataset.\n",
    "            num_workers : Number of workers to support the dataloader, default is 0.\n",
    "            pin_memory : Default is False. Pinning memory makes data loading efficent when a accelerator is used and num of workers>0.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            Torch dataloader objects for training,testing and validation data.\n",
    "            \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        ## Create an object of the data preparation class\n",
    "        dataprep = DataPreparation(H=self.H,W=self.W,data_dir=self.base_data_dir,device = self.device, perform_standardization = perform_standardization)\n",
    "\n",
    "        if (not self.device == \"cpu\") and num_workers>0:\n",
    "            pin_memory = True\n",
    "\n",
    "        ## create a train dataset loader\n",
    "        self.train_loader = dataprep.create_dataloader(sub_dir = \"train/\",batch_size = batch_size,shuffle = shuffle, num_workers = num_workers,data_augmentation_transforms = list_of_train_data_augmentation_transforms,pin_memory=pin_memory)\n",
    "\n",
    "        ## it is not efficient to pin memory for validation and test datasets,as they are relatively small.\n",
    "        self.val_loader = dataprep.create_dataloader(sub_dir = \"validation/\",batch_size = batch_size,shuffle = shuffle, num_workers = num_workers,pin_memory=False)\n",
    "        self.test_loader = dataprep.create_dataloader(sub_dir = \"test/\",batch_size = batch_size,shuffle = shuffle, num_workers = num_workers,pin_memory=False)\n",
    "\n",
    "        return self.train_loader,self.val_loader,self.test_loader\n",
    "\n",
    "\n",
    "    def createCNNmodel(self,num_output_neurons, conv_activation,fc_activation ,convolution_layer_specifications, hidden_layer_specifications, output_activation, conv_batch_norm, fc_batch_norm, dropout ,W, H, input_depth):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create CNN model with the given configuration for the experiment.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            num_output_neurons: Number of neurons in the output layer.\n",
    "\n",
    "            conv_activation : A torch nn method to be used as activation function after the convolutional layers.\n",
    "            \n",
    "            fc_activation : A torch nn method to be used as activation function for the fully connected layers.\n",
    "            \n",
    "            convolution_layer_specifications: A list of lists. There exists one list per conv or maxpool. \n",
    "\n",
    "                        The first element of the list is a string \"conv\" or \"maxpooling\". Indicating the layer type\n",
    "                \"conv\" is followed by number of filters, filter sizes, stride, paddings. It is assumed that every convolutional layer is followed by an activation layer.\n",
    "                \"maxpool\" is followed by filter size and stride.\n",
    "            \n",
    "            hidden_layer_specifications: A list of ints. Number of elements correspond to number of hidden layers and each value gives the number of neurons in the corresponding hidden layer.\n",
    "            \n",
    "            output_activation: The torch nn activation method to be used for the output layer.\n",
    "\n",
    "            batch_norm  : Applies batch norm after each Convolutional Layer or Fully Connected layer.\n",
    "\n",
    "            dropout :  The dropout probability (float) or None.\n",
    "\n",
    "            H : Height of the input image. [should be fixed for the dataset] \n",
    "\n",
    "            W : Width of the input image.  [should be fixed for the dataset]\n",
    "\n",
    "            input_depth : Depth of the input image (number of channels). [should be fixed for the dataset]\n",
    "\n",
    "\n",
    "            Note:\n",
    "                Dropout is a regularization method, and Convolutional layers by construction have sparse and shared weights so they have regularization inbuilt,\n",
    "\n",
    "                hence dropout is applied only to the fully connected layers.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "            None.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = CNN(num_output_neurons=num_output_neurons, conv_activation=conv_activation,fc_activation = fc_activation ,convolution_layer_specifications=convolution_layer_specifications, hidden_layer_specifications=hidden_layer_specifications, output_activation = output_activation, conv_batch_norm = conv_batch_norm, fc_batch_norm = fc_batch_norm, dropout = dropout ,W=W, H=H, input_depth=input_depth)\n",
    "        self.model.apply(self.model.initalize_weights_biases)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def compute_accuracy(self,model,data_iterator):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to compute the accuracy of the given model over the dataset in the data_iterator.\n",
    "\n",
    "        params:\n",
    "\n",
    "            model : The torch neural net model whose performance has to be measured.\n",
    "\n",
    "            data_iterator : The data iterator over which the computation of the metrics has to be done.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            loss,accuracy of the \"model\" over the \"data_iterator\".\n",
    "        \"\"\"\n",
    "    \n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "    \n",
    "        loss = 0\n",
    "        train_mode = model.training\n",
    "        \n",
    "        # since we're testing, switch of train mode if it is on.\n",
    "        if train_mode:\n",
    "            model.eval()\n",
    "    \n",
    "        with torch.no_grad(): ##don't compute gradients\n",
    "            for data in data_iterator:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device) ## move the inputs and labels to the device\n",
    "        \n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = model(images)\n",
    "                loss += self.criterion(outputs, labels).item() * images.size(0) ## loss.item() is average loss of the batch, so multiply by batch size.\n",
    "                \n",
    "                preds = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                total_preds += images.size(0)\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "        \n",
    "        if train_mode: # if model was originally in train mode, switch it back to train mode.\n",
    "            model.train() ## switch back to train mode\n",
    "    \n",
    "        #print(f'Accuracy of the model on the {len(data_iterator.dataset.samples)} test images: {round(100*correct/total,2)} %')\n",
    "    \n",
    "        accuracy = round(100*correct_preds/total_preds,2)\n",
    "        loss = round(loss/total_preds,2)\n",
    "        \n",
    "        return loss,accuracy\n",
    "\n",
    "    def train(self,lr,weight_decay,loss,optimiser,epochs):\n",
    "\n",
    "        \"\"\"\n",
    "        The method to perform the training, assuming model is already created using createCNNmodel method.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            lr : Learning rate\n",
    "            weight_decay : l2 regularization parameter.\n",
    "            loss : string, loss type. currently only \"crossentropy\" is supported\n",
    "            optimiser : \"adam\",\"nadam\",\"rmsprop\".\n",
    "            epochs : number of epochs to train.\n",
    "            \n",
    "        Returns:\n",
    "\n",
    "            None.\n",
    "        \"\"\"\n",
    "\n",
    "        ## specify the optimiser\n",
    "        if optimiser.lower() == \"adam\":\n",
    "            self.optimiser = optim.Adam(self.model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "        elif optimiser.lower() == \"nadam\":\n",
    "            self.optimiser = optim.NAdam(self.model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "        elif optimiser.lower() == \"rmsprop\":\n",
    "            self.optimiser = optim.RMSprop(self.model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "        ## Specify the loss criteria\n",
    "        if loss.lower() == \"crossentropy\":\n",
    "           self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        ## loop over the dataset multiple times\n",
    "        for epoch in tqdm(range(epochs)):  \n",
    "            \n",
    "            correct_preds = 0\n",
    "            total = 0\n",
    "            count = 0\n",
    "            epoch_loss = 0.0\n",
    "        \n",
    "            \n",
    "            for i, data in enumerate(self.train_loader):\n",
    "                \n",
    "                ## i is batch index\n",
    "                \n",
    "                images, labels = data[0].to(self.device),data[1].to(self.device)  ## move the images and labels to the device.\n",
    "        \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                self.optimiser.zero_grad()\n",
    "            \n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(images).to(self.device)\n",
    "                \n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "            \n",
    "                epoch_loss +=  loss.item() * images.size(0) ## loss.item() is average loss of the batch, so multiply by batch size.\n",
    "                \n",
    "                preds = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                total += images.size(0)\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "\n",
    "\n",
    "            train_accuracy = round(100*correct_preds/total,2)\n",
    "            train_loss = epoch_loss/total\n",
    "            \n",
    "            val_loss,val_accuracy = self.compute_accuracy(self.model,self.val_loader)\n",
    "        \n",
    "            if epoch%5 == 0:\n",
    "\n",
    "\n",
    "                if self.on_kaggle:\n",
    "                    torch.save(model, \"/kaggle/working/model\")\n",
    "                else:\n",
    "                    torch.save(self.model, \"Model\")\n",
    "                \n",
    "        \n",
    "            if epoch == 0:\n",
    "                print(f\"Samples in Train Data : {total}\")\n",
    "\n",
    "            if self.wandb_logging:\n",
    "\n",
    "                wandb.log({'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': train_loss, 'Validation accuracy': train_accuracy,'epoch': epoch+1})\n",
    "\n",
    "            print(f'Epoch : {epoch+1}\\t Train Accuracy : {train_accuracy:.2f}%\\t Train loss: {train_loss:.2f}\\t Validation Accuracy : {val_accuracy:.2f}%\\t Validation Loss : {val_loss:.2f}')\n",
    "            epoch_loss = 0.0\n",
    "        \n",
    "        print('Finished Training!!')\n",
    "        \n",
    "        end_time = time.time() - start_time\n",
    "        print(f\"Time Taken for Training: {round(end_time/60,2)}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m036\u001b[0m (\u001b[33mtmajestical\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/tejasmalladi/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 1n5js8td\n",
      "Sweep URL: https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/sweeps/1n5js8td\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"\")\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name' : 'PA2 Initial Trials',\n",
    "    'metric': {\n",
    "      'name': 'Validation accuracy',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'num_hidden_layers': {\n",
    "            'values': [1,2,3]\n",
    "        },    \n",
    "         'hidden_size':{\n",
    "            'values':[32,64,128]\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['relu','silu','tanh']\n",
    "        },\n",
    "\n",
    "        'optimiser': {\n",
    "            'values': [\"adam\",\"rmsprop\",\"nadam\"]\n",
    "        },\n",
    "        \n",
    "        \n",
    "        'lr': {\n",
    "            'values': [1e-3,1e-4,3e-4]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0,1e-4,1e-3]\n",
    "        },\n",
    "\n",
    "        'data_aug' : {\n",
    "\n",
    "            'values' : [1,2,3]\n",
    "        },\n",
    "\n",
    "        'epochs' : {\n",
    "\n",
    "            'values' : [5,10,15,20]\n",
    "        }\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='JV_CS23M036_TEJASVI_DL_ASSIGNMENT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_start_expt(config,wandb_log=False,kaggle = False):\n",
    "    ##using apple silicon GPU\n",
    "\n",
    "    wandb_logging = wandb_log\n",
    "    on_kaggle = kaggle\n",
    "\n",
    "    if on_kaggle:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        base_data_dir = \"/kaggle/input/jv-inaturalist-dataset/inaturalist_12K/\"\n",
    "    else:\n",
    "        device = \"mps\"\n",
    "        base_data_dir = \"inaturalist_12K/\"\n",
    "        \n",
    "    \n",
    "    print(f\"Using {device}\")\n",
    "\n",
    "    H = W = 800\n",
    "    \n",
    "    ## dataloader creation hyperparams:\n",
    "    \n",
    "    batch_size = 32\n",
    "    shuffle = True\n",
    "    num_workers = 3\n",
    "    perform_standardization = False\n",
    "    pin_memory = False\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    train_data_augmentation_transforms1 = [transforms.RandomPerspective(p=1)]\n",
    "    train_data_augmentation_transforms2 = [transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))]\n",
    "    train_data_augmentation_transforms3 = [transforms.ColorJitter(brightness=.5, hue=.3)]\n",
    "    list_of_train_data_augmentation_transforms = [train_data_augmentation_transforms1,train_data_augmentation_transforms3,train_data_augmentation_transforms2]\n",
    "\n",
    "    list_of_train_data_augmentation_transforms = list_of_train_data_augmentation_transforms[:config['data_aug']]\n",
    "\n",
    "    \n",
    "    \n",
    "    ## create an experiment\n",
    "    experiment = Experiment(H=H,W=H,device=device,base_dir = base_data_dir, wandb_logging=wandb_logging,on_kaggle = on_kaggle)\n",
    "    \n",
    "    \n",
    "    ##create data loaders for train, validation and test datasets.\n",
    "    train_loader,val_loader,test_loader  = experiment.create_dataloaders(batch_size=batch_size,shuffle=shuffle,perform_standardization=perform_standardization,list_of_train_data_augmentation_transforms=list_of_train_data_augmentation_transforms,num_workers=num_workers,pin_memory=pin_memory)\n",
    "    \n",
    "    \n",
    "    ## CNN Hyperparams\n",
    "    num_output_neurons =  len(val_loader.dataset.classes)\n",
    "    \n",
    "    convolution_layer_specifications = [\n",
    "    \n",
    "        [\"conv\",6,5,1,2],\n",
    "        [\"maxpool\",2,2],\n",
    "        [\"conv\",16,5,3,0],\n",
    "        [\"maxpool\",2,2],\n",
    "        [\"conv\",16,7,3,0],\n",
    "        [\"maxpool\",2,3],\n",
    "    ]\n",
    "    \n",
    "    hidden_layer_specifications = [config['hidden_size']] * config[\"num_hidden_layers\"]\n",
    "\n",
    "    if config['activation'].lower() == \"relu\":\n",
    "        fc_activation = nn.ReLU()\n",
    "\n",
    "    elif config['activation'].lower() == \"silu\":\n",
    "        fc_activation = nn.SiLU()\n",
    "\n",
    "    if config['activation'].lower() == \"tanh\":\n",
    "        fc_activation = nn.Tanh()\n",
    "        \n",
    "\n",
    "    ## fixing this as per the litearture.\n",
    "    conv_activation=nn.ReLU()\n",
    "    \n",
    "    output_activation = nn.LogSoftmax(dim=1)\n",
    "    conv_batch_norm = True\n",
    "    fc_batch_norm = True\n",
    "    dropout = 0.3\n",
    "    \n",
    "    input_depth=3\n",
    "    \n",
    "    ## create CNN model\n",
    "    experiment.createCNNmodel(num_output_neurons, conv_activation,fc_activation ,convolution_layer_specifications, hidden_layer_specifications, output_activation, conv_batch_norm, fc_batch_norm, dropout ,W, H, input_depth)\n",
    "    \n",
    "    ## To see outputs of convolution and maxpool layers\n",
    "    #experiment.model.convolution_dimensions(input_channels = input_depth)\n",
    "    \n",
    "    ##training Hyper Params:\n",
    "    \n",
    "    lr = config['lr']\n",
    "    weight_decay = config['weight_decay']\n",
    "    optimiser = config['optimiser']\n",
    "    epochs = config['epochs']\n",
    "    loss = \"crossentropy\"\n",
    "    \n",
    "    experiment.train(lr = lr,weight_decay = weight_decay,loss = loss,optimiser = optimiser,epochs = epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qhem7e75 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_aug: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tejasmalladi/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment2/wandb/run-20240401_224755-qhem7e75</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/runs/qhem7e75' target=\"_blank\">desert-sweep-1</a></strong> to <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/sweeps/1n5js8td' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/sweeps/1n5js8td</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/sweeps/1n5js8td' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/sweeps/1n5js8td</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/runs/qhem7e75' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/runs/qhem7e75</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n",
      "Preparing data from train/\n",
      "Preparing data from validation/\n",
      "Preparing data from test/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6022c54ed0764faebf1115074420f876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in Train Data : 23997\n",
      "Epoch : 1\t Train Accuracy : 12.98%\t Train loss: 2.50\t Validation Accuracy : 19.85%\t Validation Loss : 2.21\n",
      "Finished Training!!\n",
      "Time Taken for Training: 3.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Validation accuracy</td><td>▁</td></tr><tr><td>Validation loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁</td></tr><tr><td>train loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Validation accuracy</td><td>12.98</td></tr><tr><td>Validation loss</td><td>2.49629</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>train accuracy</td><td>12.98</td></tr><tr><td>train loss</td><td>2.49629</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">desert-sweep-1</strong> at: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/runs/qhem7e75' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT2/runs/qhem7e75</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240401_224755-qhem7e75/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    '''\n",
    "    WandB calls main function each time with differnet combination.\n",
    "\n",
    "    We can retrive the same and use the same values for our hypermeters.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    with wandb.init() as run:\n",
    "\n",
    "        run_name=\"-hl_\"+str(wandb.config.num_hidden_layers)+\"-hs_\"+str(wandb.config.hidden_size)+\"-ac_\"+str(wandb.config.activation)\n",
    "\n",
    "        run_name = run_name+\"-optim_\"+str(wandb.config.optimiser)+\"-lr_\"+str(wandb.config.lr)+\"-reg_\"+str(wandb.config.weight_decay)+\"-epochs_\"+str(wandb.config.epochs)+\"-data_aug\"+str(wandb.config.data_aug)\n",
    "\n",
    "        wandb.run.name=run_name\n",
    "\n",
    "        setup_and_start_expt(wandb.config,wandb_log = True,kaggle = False)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=main,count=1) # calls main function for count number of times.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "2. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#:~:text=PyTorch%20provides%20two%20data%20primitives,easy%20access%20to%20the%20samples.\n",
    "3. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel [to create dataloaders]\n",
    "4. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html [To understand overall structure of torch code]\n",
    "5. https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html [for composing transform]\n",
    "6. https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html [for resizing images]\n",
    "7. https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize [For normalization]\n",
    "8. https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py [For data augmentation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4675321,
     "sourceId": 7950185,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
