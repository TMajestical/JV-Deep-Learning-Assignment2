{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0816db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5a6f3",
   "metadata": {},
   "source": [
    "I would develop the code for the assignment in this notebook as it is easy to quickly test (and even unit testing).\n",
    "\n",
    "When a module/part is bug free I would add it to the .py file later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f10cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "#from torchvision.transforms import v2 as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f619250",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 76 #setting this as seed wherever randomness comes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da9041e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSome important points:\\n\\n1. Images are of different shapes, so they all have to be resized by looking at the image shapes and picking the maximum [or mean/mode ?] width and height.\\n\\n2. Now they have to be normalized. To zero mean and unit variance. This has to be done within train, test and validation datasets separately. One normalization won't work for the other because the images may go out of scale (i.e beyond +/- 1)\\n\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Some important points:\n",
    "\n",
    "1. Images are of different shapes, so they all have to be resized by looking at the image shapes and picking the maximum [or mean/mode ?] width and height.\n",
    "\n",
    "2. Now they have to be normalized. To zero mean and unit variance. This has to be done within train, test and validation datasets separately. One normalization won't work for the other because the images may go out of scale (i.e beyond +/- 1)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efca418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sizes(ds):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(len(ds.samples)):\n",
    "        \n",
    "        x,y = train_dataset[i][0].shape[1:]\n",
    "    \n",
    "        x_vals.append(x)\n",
    "        y_vals.append(y)\n",
    "        \n",
    "        #images.append(train_dataset[i][0])\n",
    "\n",
    "    print(f\"X min : {np.min(x_vals)}\\tX Max:{np.max(x_vals)}\")    \n",
    "    print(f\"Y min : {np.min(y_vals)}\\tY Max:{np.max(y_vals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09edf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_image_sizes(train_dataset)\n",
    "#get_image_sizes(val_dataset)\n",
    "#get_image_sizes(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aae7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs the maximum dimensions are 800,800 for train, validatio and test data\\n\\nResizing all images to 800x800 size.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "As the maximum dimensions are 800,800 for train, validatio and test data\n",
    "\n",
    "Resizing all images to 800x800 size.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa299ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b9aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4322cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class (that inherits nn.Module), to create the CNN architecture as required and to define the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_output_dimension_post_convolutions(self,convolution_layer_specifications):\n",
    "            \n",
    "            \"\"\"\n",
    "            Method to compute the dimension of the output after all convolution,maxpooling layers.\n",
    "            This helps to compute the input size of first fully connected layer.\n",
    "\n",
    "            Params:\n",
    "                convolution_layer_specifications :  A list of lists. There exists one list per conv or maxpool. \n",
    "\n",
    "                        The first element of the list is a string \"conv\" or \"maxpooling\". Indicating the layer type\n",
    "                \"conv\" is followed by number of filters, filter sizes, stride, paddings. It is assumed that every convolutional layer is followed by an activation layer.\n",
    "                \"maxpool\" is followed by filter size and stride.\n",
    "            \n",
    "            Returns:\n",
    "                A tuple containing the dimension (Height,Width,depth) of the output after all convolutional operations.\n",
    "            \"\"\"\n",
    "\n",
    "            W_cur = self.W\n",
    "            H_cur = self.H\n",
    "            depth_cur = self.input_channels\n",
    "            \n",
    "            for config in convolution_layer_specifications: ## assuming square filters\n",
    "            \n",
    "                layer_type = config[0]\n",
    "            \n",
    "                if layer_type == \"conv\":  ## its a convolutional layer\n",
    "            \n",
    "                    \n",
    "                    _, number_of_filters, filter_size, stride, padding = config\n",
    "                    \n",
    "                    W_cur = (W_cur - filter_size + 2*padding)//stride + 1\n",
    "                    H_cur = (H_cur - filter_size + 2*padding)//stride + 1\n",
    "                    depth_cur = number_of_filters\n",
    "                    \n",
    "                elif layer_type == \"maxpool\":\n",
    "                    _,max_pool_filter_size,max_pool_stride = config\n",
    "            \n",
    "                    W_cur = (W_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "                    H_cur = (H_cur-max_pool_filter_size)//max_pool_stride + 1\n",
    "            \n",
    "\n",
    "            return (H_cur,W_cur,depth_cur)\n",
    "\n",
    "    \n",
    "    def __init__(self,num_output_neurons, conv_activation, fc_activation, convolution_layer_specifications, hidden_layer_specifications, output_activation,conv_batch_norm = False,fc_batch_norm = False, W=800, H=800, input_depth=3):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            num_output_neurons: Number of neurons in the output layer.\n",
    "\n",
    "            conv_activation : A torch nn method to be used as activation function after the convolutional layers.\n",
    "            \n",
    "            fc_activation : A torch nn method to be used as activation function for the fully connected layers.\n",
    "            \n",
    "            convolution_layer_specifications: A list of lists. There exists one list per conv or maxpool. \n",
    "\n",
    "                        The first element of the list is a string \"conv\" or \"maxpooling\". Indicating the layer type\n",
    "                \"conv\" is followed by number of filters, filter sizes, stride, paddings. It is assumed that every convolutional layer is followed by an activation layer.\n",
    "                \"maxpool\" is followed by filter size and stride.\n",
    "            \n",
    "            hidden_layer_specifications: A list of ints. Number of elements correspond to number of hidden layers and each value gives the number of neurons in the corresponding hidden layer.\n",
    "            \n",
    "            output_activation: The torch nn activation method to be used for the output layer.\n",
    "\n",
    "            batch_norm  : Applies batch norm after each Convolutional Layer or Fully Connected layer.\n",
    "\n",
    "            H : Height of the input image. [should be fixed for the dataset] (default:800)\n",
    "\n",
    "            W : Width of the input image.  [should be fixed for the dataset]  (default:800)\n",
    "\n",
    "            input_depth : Depth of the input image (number of channels). [should be fixed for the dataset] (default:3)\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "            None.\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.input_channels = input_depth\n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "        self.convolutional_layers = nn.ModuleList() ## Create a module list to organize the convolutional layers\n",
    "        \n",
    "        \n",
    "        ## iterate over the convolution_layer_specifications and create the convolutional layers accordingly\n",
    "\n",
    "        cur_depth = self.input_channels\n",
    "        for config in convolution_layer_specifications:\n",
    "\n",
    "            ## Assuming filter is a square matrix, so filter_size is int.\n",
    "            type = config[0]\n",
    "\n",
    "            if type == \"conv\":\n",
    "                _, number_of_filters, filter_size, stride, padding = config\n",
    "                self.convolutional_layers.append(nn.Conv2d(cur_depth, number_of_filters, filter_size, stride, padding))\n",
    "                ## If batchnorm is to be done for Conv layers\n",
    "                if conv_batch_norm:\n",
    "                    self.convolutional_layers.append(nn.BatchNorm2d(number_of_filters))\n",
    "                self.convolutional_layers.append(conv_activation)  # Add activation after each convolutional layer\n",
    "            \n",
    "            elif type == \"maxpool\":\n",
    "                _,max_pool_filter_size,max_pool_stride = config\n",
    "                self.convolutional_layers.append(nn.MaxPool2d(max_pool_filter_size,max_pool_stride))\n",
    "            \n",
    "            cur_depth = number_of_filters  # Update input depth for next layer\n",
    "\n",
    "        self.convolutional_layers.apply(self.initalize_weights_biases)\n",
    "        \n",
    "        ## iterate over the hidden_layer_specifications and create CNN layers accordingly.\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        conv_output_height,conv_output_height,conv_output_depth = self.compute_output_dimension_post_convolutions(convolution_layer_specifications)\n",
    "        \n",
    "        fan_in =  conv_output_height*conv_output_height*conv_output_depth ## interface betweeon maxpooling and dense layer\n",
    "        \n",
    "        for hidden_size in hidden_layer_specifications:\n",
    "            self.hidden_layers.append(nn.Linear(fan_in, hidden_size))\n",
    "            ## If batchnorm is to be done for FC layers\n",
    "            if fc_batch_norm:\n",
    "                self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            self.hidden_layers.append(fc_activation)  # Add ReLU activation after each dense layer\n",
    "            fan_in = hidden_size  # Update number of input features for next layer\n",
    "\n",
    "        self.hidden_layers.apply(self.initalize_weights_biases)\n",
    "\n",
    "        self.output = nn.ModuleList()\n",
    "        self.output.append(nn.Linear(hidden_layer_specifications[-1], num_output_neurons))\n",
    "        self.output.append(self.output_activation)\n",
    "        \n",
    "        self.output.apply(self.initalize_weights_biases)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        ## pass through convolution, activation, pooling layer set\n",
    "        for layer in self.convolutional_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "        ## pass through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        for layer in self.output: ## compute output and apply softmax\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "    def initalize_weights_biases(self,m):\n",
    "        \"\"\"\n",
    "        \n",
    "        Method to initialize weights given a torch module.\n",
    "\n",
    "        Using \"HE\" (kaiming_normal) Initialization, as it goes well with ReLU in CNN.\n",
    "        \n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):  ## it its a fully connected layer\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.01) ##a small non-zero value\n",
    "\n",
    "        elif isinstance(m, nn.Conv2d): ## if its a Convolutional layer\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.01) ##a small non-zero value\n",
    "\n",
    "        ## Maxpool and activation layers would not have any parameters, so no initialization for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76164a5c-d79b-4d2a-b2e9-f19ec5a3bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "\n",
    "    def __init__(self,H,W,data_dir,device):\n",
    "\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.base_dir  = data_dir\n",
    "        self.device = device\n",
    "\n",
    "    def compute_mean_and_dev_in_dataset(self,sub_dir):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to compute the channel wise mean and std dev in the orignal (train/validation/test) dataset.\n",
    "\n",
    "        params:\n",
    "        \n",
    "            sub_dir : \"train/\" or \"validation/\" or \"test/\", from which data has to be taken.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            mean,std of the dataset.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(self.H,self.W)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "        dataset = torchvision.datasets.ImageFolder(root=self.base_dir+sub_dir,transform=data_transforms)\n",
    "        loader = torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=False,num_workers=3)\n",
    "    \n",
    "        mean = 0.0\n",
    "        var = 0.0\n",
    "        count = 0\n",
    "    \n",
    "        for _, data in enumerate(loader, 0):\n",
    "    \n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device) ## move the inputs and lables to the device.\n",
    "    \n",
    "            # Reshape inputs to be the shape of [B, C, W * H]\n",
    "            # where B is the batch size, C is the number of channels in the image, and W and H are the width and height of the image respectively\n",
    "            inputs = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "    \n",
    "            # Update total number of images\n",
    "            count += inputs.size(0)\n",
    "    \n",
    "            # Compute mean and std here\n",
    "            mean += inputs.mean(2).sum(0)\n",
    "            var += inputs.var(2).sum(0)\n",
    "        \n",
    "        mean /= count\n",
    "        var /= count\n",
    "        std = torch.sqrt(var)\n",
    "    \n",
    "        #print(mean,std)\n",
    "        \n",
    "        return mean.cpu(),std.cpu()\n",
    "\n",
    "    def create_dataloader(self,sub_dir,batch_size=32,shuffle=True,num_workers=2,data_augmentation_transforms = None):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create dataset and return dataloader after applying all necessary transforms.\n",
    "\n",
    "        params:\n",
    "\n",
    "            sub_dir : \"train/\" or \"validation/\" or \"test/\"\n",
    "            batch_size : The batch size in which training has to be performed.\n",
    "            shuffle : whether shuffling must be done before sampling.\n",
    "            num_works : Number of workers to be used on the dataset.\n",
    "            data_augmentation_transforms ; Either None or List of transforms to be applied for data agumentation.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            Dataloader corresponding to the dataset.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Preparing data from {sub_dir}\")\n",
    "\n",
    "        data_transforms_list = [transforms.Resize(size=(self.H,self.W)),transforms.ToTensor()]\n",
    "\n",
    "        if data_augmentation_transforms:\n",
    "            data_transforms_list = data_transforms_list + data_augmentation_transforms\n",
    "\n",
    "\n",
    "        mean,std = self.compute_mean_and_dev_in_dataset(sub_dir)\n",
    "        \n",
    "        data_transforms_list = data_transforms_list + [transforms.Normalize(mean,std)]\n",
    "\n",
    "        self.dataset = torchvision.datasets.ImageFolder(root=self.base_dir+sub_dir,transform=transforms.Compose(data_transforms_list))\n",
    "        \n",
    "        self.loader = torch.utils.data.DataLoader(self.dataset,batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "            \n",
    "        return self.loader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd67637-7a34-4cd2-bc52-1bc92308156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(model,data_iterator):\n",
    "    \n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    loss = 0\n",
    "    train_mode = model.training\n",
    "    \n",
    "        # since we're testing, switch of train mode if it is on.\n",
    "    if train_mode:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad(): ##don't compute gradients\n",
    "        for data in data_iterator:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device) ## move the inputs and labels to the device\n",
    "    \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total_preds += len(images)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "    \n",
    "    if train_mode: # if model was originally in train mode, switch it back to train mode.\n",
    "        model.train() ## switch back to train mode\n",
    "\n",
    "    #print(f'Accuracy of the model on the {len(data_iterator.dataset.samples)} test images: {round(100*correct/total,2)} %')\n",
    "\n",
    "    accuracy = round(100*correct_preds/total_preds,2)\n",
    "    loss = round(loss/total_preds,2)\n",
    "    \n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce4570d9-f5e7-4a16-a917-d4ba06d7a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_network_architecture(model):\n",
    "    print(model)\n",
    "    print(\"Network Architecture:\")\n",
    "    for name, module in model.named_children():\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Layer Name: {name}\")\n",
    "        print(module)\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fbcf837-b62f-4b48-8e4a-cac4ebfd699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n",
      "Preparing data from train/\n",
      "Preparing data from validation/\n",
      "Preparing data from test/\n"
     ]
    }
   ],
   "source": [
    "## Create dataloaders\n",
    "\n",
    "\n",
    "##using apple silicon GPU\n",
    "device = \"mps\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "H = W = 800\n",
    "\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "num_workers = 3\n",
    "base_data_dir = \"inaturalist_12K/\"\n",
    "\n",
    "dataprep = DataPreparation(H=H,W=W,data_dir=base_data_dir,device = device)\n",
    "\n",
    "train_data_augmentation_transforms = [transforms.RandomPerspective(),transforms.RandomRotation(degrees=(0, 180)),transforms.ColorJitter()]\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    Note : \n",
    "\n",
    "        In random data augmentation, probabilities of each random operation is 0.5, hence in an expected sense, the class balance would still hold.\n",
    "            \n",
    "        \n",
    "        The mean and stddev would change after random data augmentation, and are known only after applying specifying the transforms for the dataset.\n",
    "        So these values will not be known while actually applying the normalization hence as a substitute considering the mean and std of the original dataset.\n",
    "\n",
    "        And without data augmentation these values would be accurate.\n",
    "\"\"\"\n",
    "\n",
    "train_loader = dataprep.create_dataloader(\"train/\",batch_size = batch_size,shuffle = shuffle, num_workers = num_workers,data_augmentation_transforms = train_data_augmentation_transforms)\n",
    "\n",
    "val_loader = dataprep.create_dataloader(\"validation/\",batch_size = batch_size,shuffle = shuffle, num_workers = num_workers)\n",
    "\n",
    "test_loader = dataprep.create_dataloader(\"test/\",batch_size = batch_size,shuffle = shuffle, num_workers = num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f589ca-5dce-4cfa-899f-c87566f524ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (output_activation): Softmax(dim=1)\n",
       "  (convolutional_layers): ModuleList(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(16, 16, kernel_size=(7, 7), stride=(3, 3))\n",
       "    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=400, out_features=128, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): SELU()\n",
       "  )\n",
       "  (output): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create model\n",
    "\n",
    "num_classes =  len(train_loader.dataset.classes)\n",
    "\n",
    "convolution_layer_specifications = [\n",
    "\n",
    "    [\"conv\",6,5,2,0],\n",
    "    [\"maxpool\",2,2],\n",
    "    [\"conv\",16,5,2,0],\n",
    "    [\"maxpool\",2,2],\n",
    "    [\"conv\",16,7,3,0],\n",
    "    [\"maxpool\",2,3],\n",
    "]\n",
    "\n",
    "hidden_layer_specifications = [128,64]\n",
    "\n",
    "W = H = 800\n",
    "\n",
    "#### number of rows = batch_size and cols = 10, so apply softmax for each row. so dim=1 for softmax.\n",
    "\n",
    "model = CNN(num_output_neurons=num_classes, conv_activation=nn.ReLU(),fc_activation = nn.SELU() ,convolution_layer_specifications=convolution_layer_specifications, hidden_layer_specifications=hidden_layer_specifications, output_activation = nn.Softmax(dim=1), conv_batch_norm = True, fc_batch_norm = False ,W=W, H=H, input_depth=3)\n",
    "model.apply(model.initalize_weights_biases)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay = 1e-3)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    \n",
    "    #print(f\"Epoch:{epoch+1}\")\n",
    "    correct_preds = 0\n",
    "    total = 0\n",
    "    count = 0\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) ## move the inputs and lables to the device.\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs).to(device)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss +=  loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct_preds += (preds == labels).sum().item()\n",
    "\n",
    "        \n",
    "        #print(count)\n",
    "\n",
    "    val_loss,val_accuracy = compute_accuracy(model,val_loader)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(f'Epoch : {epoch+1}\\t Train Accuracy : {round(100*correct_preds/total,2)}%\\t Train loss: {epoch_loss/total:.2f}\\t Validation Loss : {val_loss}\\t Validation Accuracy : {val_accuracy}%')\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "print('Finished Training!!')\n",
    "\n",
    "\n",
    "print(f\"Time Taken : {round(end_time/60,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9376f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To add:\n",
    "\n",
    "Dropout.\n",
    "early stopping with patience\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5db22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e806896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a14dacb6",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "2. https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#:~:text=PyTorch%20provides%20two%20data%20primitives,easy%20access%20to%20the%20samples.\n",
    "3. https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel [to create dataloaders]\n",
    "4. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html [To understand overall structure of torch code]\n",
    "5. https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html [for composing transform]\n",
    "6. https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html [for resizing images]\n",
    "7. https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize [For normalization]\n",
    "8. https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py [For data augmentation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd38388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4675321,
     "sourceId": 7950185,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
